## 非线性回归

用线性回归模型做非线性回归，就需要用多项式变换和sin变换来处理特征值数据，从而达到非线性拟合的效果

#### 1、包导入

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from linear_regression import LinearRegression
```

#### 2、数据读取与展示

[数据下载](https://github.com/monarch-beluga/Study-Code/blob/master/Python/MachineLearning/data/non-linear-regression-x-y.csv)

```python
data = pd.read_csv('../data/non-linear-regression-x-y.csv')

x = data['x'].values.reshape((data.shape[0], 1))
y = data['y'].values.reshape((data.shape[0], 1))

plt.plot(x, y)
plt.show()
```

![image-20240105235243678](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240105235245750-1092508151.png)

#### 3、模型训练

```python
# 非线性回归就需要用到 polynomial 和 sinusoid 变换
# polynomial_degree 和 sinusoid_degree大小的选择一般是不确定的，
# 要通过不断调试，来得到一个好的值，太小会欠拟合，太大又容易过拟合
num_iterations = 50000
learning_rate = 0.02
polynomial_degree = 15
sinusoid_degree = 15
normalize_data = True

linear_regression = LinearRegression(x, y, polynomial_degree, sinusoid_degree, normalize_data)
```

- 这里我们迭代次数选择了50000次，且使用了15阶多项式和sin函数变换，计算量比较大

#### 4、获取损失，并绘制损失变化

```python
theta, cost_history = linear_regression.train(learning_rate, num_iterations)

print('开始损失: {:.2f}'.format(cost_history[0]))
print('结束损失: {:.2f}'.format(cost_history[-1]))

plt.plot(range(num_iterations), cost_history)
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.title('Gradient Descent Progress')
plt.show()
```

![image-20240105235610766](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240105235612671-1822852450.png)

![image-20240105235629379](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240105235631344-1875366166.png)

#### 5、绘制回归曲线

```python
predictions_num = 1000
x_predictions = np.linspace(x.min(), x.max(), predictions_num).reshape(predictions_num, 1);
y_predictions = linear_regression.predict(x_predictions)

plt.plot(x, y, label='Training Dataset')
plt.plot(x_predictions, y_predictions, 'r', label='Prediction')
plt.show()
```

![image-20240105235710016](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240105235711951-1193210584.png)

