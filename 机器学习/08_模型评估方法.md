## 模型评估方法

用一些方法来评估模型的好坏，这里我们学习一下随机梯度下降分类器和交叉验证以及混淆矩阵

#### 1、导入工具包

```python
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import warnings

# matplotlib参数设置
matplotlib.rcParams['axes.labelsize'] = 14
matplotlib.rcParams['xtick.labelsize'] = 12
matplotlib.rcParams['ytick.labelsize'] = 12
warnings.filterwarnings('ignore')
# 设定了单例RandomState实例的种子
np.random.seed(42)
```

- numpy 做数据处理
- matplotlib 做图像展示
- sklearn 是机器学习最常用的库，里面有大量的机器学习算法和处理工具，以及包含一些内置的数据集，可供学习使用
- warnings 用来忽略一些警告性报错



#### 2、数据集读取与处理

我们这里使用的是sklearn中内置的一个手写数字数据集mnist,mnist是(28,28,1)的灰度图像数据

```python
from sklearn.datasets import fetch_openml
mnist = fetch_openml('mnist_784', parser='auto')
X = mnist['data'].values
y = mnist['target'].values.codes
print(X.shape, y.shape)
```

- X 为特征值数据，维度为(70000, 784)，即一共有70000个样本，每个样本有784(28*28)个特征
- y 为目标数据 

```python
# 展示第一个样本
x_0 = X[0, :].reshape(28, 28)
plt.imshow(x_0, cmap="gray_r")
plt.axis('off')
plt.show()
```

- plt.imshow(): 用numpy二维数组进行绘图
  - camp为配色方案

展示第一个样本数据

![image-20240107103137369](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240107103138592-1371342322.png)

```python
X_train = X[:60000]
X_test = X[60000:]
y_train = y[:60000]
y_test = y[60000:]
```

划分训练数据和评估测试数据

```python
# 数据洗牌
# permutation随机排列序列
shuffle_index = np.random.permutation(60000)
X_train = X_train[shuffle_index]
y_train = y_train[shuffle_index]
```

数据洗牌，为了让每个样本尽量表现独立的特性



#### 3、模型训练与预测

```python
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(max_iter=5, random_state=42)
sgd_clf.fit(X_train, y_train)

# 进行预测并展示
y_pred_0 = sgd_clf.predict([X_test[3500]])
print("预测的值：", y_pred_0, "实际的值：", y_test[3500])
x_3500 = X_test[3500].reshape(28, 28)
plt.imshow(x_3500, cmap="gray_r")
plt.axis('off')
plt.show()
```

- SGDClassifier：随机梯度下降分类器，本次主要讲解评估方法，所以当作例子讲解，只介绍一些重要参数
  - loss: 损失函数
  - penalty: 惩罚函数,正则化
  - max_iter: 迭代次数
  - random_state: 指定随机种子，保证每次随机策略结果一致，方便展示
  - n_jobs：调用cpu并行数，如果为-1则调用所有cpu进行并行计算
- fit()：模型训练函数，需要传递一个形状为(n,m)的特征数据和一个形状为(n,)的目标值数据
- predict()：预测函数，需要传递一个形状为(n0,m)的特征数据，返回形状为(n0,)的预测结果

![image-20240122095827043](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240122095828524-745273033.png)





#### 4、交叉验证方法

![image-20240107105014501](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240107105020637-1876360658.png)

- 在数据建模中，我们首先把数据集分为两部分：训练集和测试集，而训练集中，再分出一小部分作为验证集，来验证每一次训练的结果。

- 选定验证集是比较重要的，如果作为验证集的那一部分数据比较复杂或过于简单，那么模型的评估就会比较差，所以引入了交叉验证的概念

- 交叉验证(Cross Validation)，通俗的来说，就是把数据分为n份，轮流将其中n-1份数据做训练建立模型，一份数据做验证，n次的结果的均值作为算法精度的估计，常用的如上图的10折交叉验证(10-fold cross validation)

  ![image-20240107110639609](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240107110640957-570074591.png)

用skleanr自带的函数进行交叉验证

```python
# 用sklearn自带的交叉验证函数
from sklearn.model_selection import cross_val_score

cross_score = cross_val_score(sgd_clf, X_train, y_train, cv=5, scoring='accuracy')
print(cross_score)
```

![image-20240122095907227](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240122095908376-60192534.png)

- cross_val_score()：交叉验证函数
  - 第一个参数为模型
  - 第二、三个参数分别为训练数据的特征数据和目标数据
  - cv：交叉验证迭代(分块)数
  - scoring：评估方法，这里我们用"accuracy"准确率作为评估方法，即预测正确样本数与所有样本数之比
  - n_jobs：调用cpu并行数，如果为-1则调用所有cpu进行并行计算

手动计算，更麻烦，但是可自定义的内容更多

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone
# 手动分块进行交叉验证
sk_folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for train_index, val_index in sk_folds.split(X_train, y_train):
    clone_clf = clone(sgd_clf)
    X_train_folds = X_train[train_index]
    y_train_folds = y_train[train_index]
    X_val_folds = X_train[val_index]
    y_val_folds = y_train[val_index]

    clone_clf.fit(X_train_folds, y_train_folds)
    y_pred = clone_clf.predict(X_val_folds)
    n_correct = sum(y_pred == y_val_folds)
    print(n_correct/len(y_pred))
```

![image-20240122095922842](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240122095923724-1684276610.png)

- StratifiedKFold()：用来分割训练数据
  - n_splits:分割块数
  - shuffle：每次分割是否打乱数据
  - random_state：随机种子
- clone(): 克隆模型，保证每次训练的模型参数是一致的



#### 5、混淆矩阵

由上面交叉验证的方法，我们可以得到预测正确与所有样本的比例即准确率(accuracy)，但是，单单这一个指标，并不能完全证明模型的好坏，假设：我们有一个模型是区分男女，一共有1000个样本，其中10个是女性，其他都是男性，有一个模型，他会把所有的人都识别成男性，那么按交叉验证，我们得到的指标是$\frac{990}{1000}$为99%，这难道能表示模型是好的吗？所以我们引入了混淆矩阵

|                         | 相关(Relevant)，正类                | 无关(NonRelevant)，负类             |
| ----------------------- | ----------------------------------- | ----------------------------------- |
| 被检索到(Retrieved)     | true positives（TP 正类判定为正类） | false positives (FP 负类判定为正类) |
| 未被检索(Not Retrieved) | false negatives (FN 正类判定为负类) | true negatives (TN 负类判定为负类)  |

举例：

已知条件：班级总人数100人，其中男生80，女生20

目标：找出所有的女生

结果：从班级中找出了35人认为是女生，其中有15人真为女生，还有20名男生被错误挑选出来

则：TP=15,FP=20,FN=5,TN=60

用sklearn来获取混淆矩阵

```python
# 混淆矩阵
# labels 指定混淆矩阵索引生成顺序的标签
labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
from sklearn.model_selection import cross_val_predict
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=5)
from sklearn.metrics import confusion_matrix
y_confusion_matrix = confusion_matrix(y_train, y_train_pred, labels=labels)
print(y_confusion_matrix)
```

![image-20240122095938313](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240122095939225-1973620771.png)



#### 6、评估指标

|                                       | 公式                                 | 意义                                       |
| ------------------------------------- | ------------------------------------ | ------------------------------------------ |
| 准确率(ACC)Accuracy                   | $Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$ | 模型正确分类样本占总样本数的比例(所有类别) |
| 精确率(PPV) Positive Predictive Value | $Precision=\frac{TP}{TP+FP}$         | 模型预测的所有正类中，预测正确的比例       |
| 召回率(TPR)True Positive Rate         | $Reall=\frac{TP}{TP+FN}$             | 所有真实正类中，模型预测正确的比例         |
| 特异度(TNR)True Negative Rate         | $Specificity=\frac{TN}{TN+FP}$       | 所有真实负类中，模型预测正确 的比例        |

将 **Precision** 和 **Recall** 进行调和平均计算，得到 **F1 score** 的指标，调和平均值给予低值更多的权重，因此，任意上述两个指标任意一个低分都会更大的影响到 **F1 score** 的得分，公式：

$F_1 = \frac{2}{\frac1{precision}+\frac1{recall}}=2 \times \frac{precision \times recall}{precision + recall} = \frac{TP}{TP+\frac{FN+FP}2}$  

```python
# 评估指标计算
TP = y_confusion_matrix.diagonal()			# diagonal 为取对角线数据
TN = TP.sum() - TP
FP = y_confusion_matrix.sum(axis=0) - TP
FN = y_confusion_matrix.sum(axis=1) - TP

# 准确率
Acc = TP.sum() / y_confusion_matrix.sum()
print("Accuracy:", Acc)

import pandas as pd
# 精确率
Ppv = TP / (TP + FP)
# 召回率
Tpr = TP / (TP + FN)
# 特异度
Tnr = TN / (TN + FP)
# F1 得分
F1 = TP / (TP + (FN + FP) / 2)
df = pd.DataFrame({'Precision': Ppv, 'Recall': Tpr, 'Specificity': Tnr}, index=labels)
print(df)
```

![image-20240122095959496](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240122100000408-1229384175.png)



#### 7、阈值对结果的影响

![2](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240121181131520-1883401030.png)

在模型预测最终结果之前，会有一个得分值，这个得分值决定了最后的结果。在二分类中，我们可以设置一个阈值，当得分达到这个阈值时，就表明是这个类别。显然，阈值的大小会对结果产生影响，最明显的就是精确率(Precision)和召回率(Recall)的改变。由上图可知，在实际运用中精确率和召回率总是不能两个都非常的高，在召回率高时，就不能保证结果的精确；在精确率高时，就不能保证结果的完全。

在模型中，阈值可以自己设置，从而达到自己想要的结果。而设置阈值的前提就是先要获取得分值

```python
# 使用模型函数来获取得分值
y_pred_scores_0 = sgd_clf.decision_function([X_test[3500]])
print(y_pred_scores_0)
```

- decision_function(): 获取得分值

![image-20240122100231401](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240122100232295-1602907069.png)

由于这个样例是十分类，所以这里会获得十个得分值，表示每一个样本在对应目标类上的得分

```python
# 在交叉验证时，获取得分值
y_train_scores = cross_val_predict(sgd_clf, X_train, y_train, cv=5, 
                                    method="decision_function")
```

由于阈值主要是体现在二分类中，所以我们要把数据进行一个one-hot变形

```python
from sklearn.preprocessing import label_binarize

y_train_one_hot = label_binarize(y_train, classes=labels)
print(y_train[0], "--->", y_train_one_hot[0])
```

![image-20240122101952913](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240122101954046-240230020.png)

将类别的标签，转化成只包含01的序列，这样序列中每一个值都能对应一个得分

```python
from sklearn.metrics import precision_recall_curve
# ravel()将二维数组展开为一维
precisions, recalls, thresholds = precision_recall_curve(y_train_one_hot.ravel(), y_train_scores.ravel())
```

- precision_recall_curve()：分别使用样本的得分作为阈值，并返回精确率、召回率和阈值

使用上述的精确率、召回率和阈值绘图，能更直观的看到精确率和召回率的负相关的关系

```python
def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):
    plt.plot(thresholds, precisions[:-1], 'b--', label="Precision")
    plt.plot(thresholds, recalls[:-1], 'g-', label="Recall")
    plt.xlabel("Threshold", fontsize=16)
    plt.legend(loc="upper left", fontsize=16)
    plt.ylim([0, 1])

plt.figure(figsize=(8, 4))
plot_precision_recall_vs_threshold(precisions, recalls, thresholds)
plt.xlim([-700000, 700000])
plt.show()
```

![image-20240122102544618](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240122102545803-1790058163.png)

```python
def plot_precision_vs_recall(precisions, recalls):
    plt.plot(recalls, precisions, 'b-', linewidth=2)
    plt.xlabel("Recall", fontsize=16)
    plt.ylabel("Precision", fontsize=16)
    plt.axis([0, 1, 0, 1])

plt.figure(figsize=(8, 6))
plot_precision_vs_recall(precisions, recalls)
plt.show()
```

![image-20240123094715646](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240123094716987-1331607253.png)



#### 8、ROC 曲线

**Recelver operating characteristic (ROC)**曲线是二元分类中的常用评估方法

- 它与精确率/召回率曲线非常相似，但ROC曲线不是绘制精确率和召回率，而是绘制**true positive rate(TPR)**与**false positive rate(FPR)**
- 要绘制ROC曲线，首先需要使用**roc_curve()**函数计算各种阈值的**TPR**和FPR

TPR = TP / (TP + FN) (Recall)

FPR = FP / (FP + TN)

```python
from sklearn.metrics import roc_curve

fpr, tpr, thresholds = roc_curve(y_train_one_hot.ravel(), y_train_scores.ravel())
```

```python
def plot_roc_curve(fpr, tpr, label=None):
    plt.plot(fpr, tpr, linewidth=2, label=label)
    plt.plot([0, 1], [0, 1], 'k--')
    plt.axis([0, 1, 0, 1])
    plt.xlabel("False Positive Rate", fontsize=16)
    plt.ylabel("True Positive Rate", fontsize=16)
    
plt.figure(figsize=(8, 6))
plot_roc_curve(fpr, tpr)
plt.show()
```

![image-20240123100953898](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240123100954830-1868789068.png)

**虚线表示纯随机分类器的ROC曲线**;一个好的分类器尽可能远离该线(朝左上角)

比较分类器的一种方法是测量曲线下面积(AUC)。完美分类器的ROC AUC 等于1，而纯随机分类器的ROC AUC等于0.5。sklearn提供了计算ROC AUC的函数：

```python
from sklearn.metrics import roc_auc_score

Roc_auc = roc_auc_score(y_train_one_hot.ravel(), y_train_scores.ravel())
print("ROC AUC:", Roc_auc)
```

![image-20240123101534300](https://img2023.cnblogs.com/blog/2213660/202401/2213660-20240123101535440-897850382.png)











